{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"text-align: center; vertical-align: middle;\">Numerical Methods of Accelerator Physics</h1>\n",
    "<h2 style=\"text-align: center; vertical-align: middle;\">Lecture Series by Dr. Adrian Oeftiger</h2>\n",
    "\n",
    "<h3 style=\"text-align: center; vertical-align: middle; margin-top: 1em; margin-bottom: 1em;\">Guest Lecture by Dr. Michael Schenk</h3>\n",
    "\n",
    "<img src=\"./img/etit.png\" style=\"width: 20%; margin: auto;\" />\n",
    "\n",
    "<h3 style=\"text-align: center; vertical-align: middle;\">Part 11: 20.01.2023</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Run this notebook online!</h2>\n",
    "\n",
    "Interact and run this jupyter notebook online:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"text-align:center;\">\n",
    "1. via the public mybinder.org service: <br />\n",
    "\n",
    "<p style=\"text-align: center; margin-left, margin-right: auto; width: 100%;\">\n",
    "<a href=\"https://mybinder.org/v2/gh/aoeftiger/TUDa-NMAP-11/v1.0\"><img src=\"./img/binder_logo.svg\" /></a>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\" style=\"text-align:center;\">\n",
    "2. on the <a href=\"https://tu-jupyter-i.ca.hrz.tu-darmstadt.de/\">local TU Darmstadt jupyterhub $\\nearrow$</a> (using your TU ID)\n",
    "\n",
    "$\\implies$ make sure you installed all the required python packages (see the [README](./README.md))!\n",
    "</div>\n",
    "\n",
    "Finally, also find this lecture rendered [as HTML slides on github $\\nearrow$](https://aoeftiger.github.io/TUDa-NMAP-11/) along with the [source repository $\\nearrow$](https://github.com/aoeftiger/TUDa-NMAP-11)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Run this first!</h2>\n",
    "\n",
    "Imports and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from config import (np, plt)\n",
    "from scipy.constants import m_p, e, c\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"color: #e6541a;\"> <strong> TO DO by Adrian</strong> </p>\n",
    "\n",
    "<h2 style=\"color: #b51f2a\">Refresher / preparation!</h2>\n",
    "\n",
    "- ...\n",
    "- Lecture by Dr. Andrea Santamaria Garcia: <a href=\"https://www.dropbox.com/s/vkqgojy81elpfs2/2023-01-KIT_SANTAMARIA_MLAcceleratorsIntro_v1.mov?dl=0\">Introduction to machine learning</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"color: #b51f2a\">Today!</h2>\n",
    "\n",
    "1. Introduction to reinforcement learning\n",
    "2. Reinforcement learning formalism\n",
    "3. Q-learning\n",
    "4. Actor-critic methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<h4>Disclaimer</h4>\n",
    "\n",
    "- Today's introduction to reinforcement learning (RL) is by no means mathematically complete\n",
    "- The idea is to give a high-level overview and some first ideas on the subject to hopefully spark your interest :)\n",
    "- RL is a fascinating field: if you want to learn more, there are some great resources at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: center; width:100%;\">\n",
    "    <h2>Part I: introduction to reinforcement learning</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Machine learning landscape</h2>\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/machine_learning_landscape.png\" alt=\"Machine learning landscape\" style=\"width: 60%;margin-top: 1cm;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">image by <a href=\"https://www.linkedin.com/pulse/business-intelligence-its-relationship-big-data-geekstyle\">GeekStyle</a></p>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Supervised learning**\n",
    "    - **Goal:** obtain mapping $F(x) = y$ using a *labelled* dataset $(x, y)$\n",
    "    - **Data:** $y$ is sometimes referred to as the ground truth or the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Unsupervised learning**\n",
    "    - **Goal:** identify structure in data\n",
    "    - **Data:** no labelled dataset is available / needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Reinforcement learning**\n",
    "    - **Goal:** find how to behave optimally in a given environment\n",
    "    - **Data:** an agent is actively interacting with the environment through trial-and-error and collects data (including positive or negative rewards)\n",
    "    - Closest to artificial intelligence: provide **minimal external input** and let the agent explore and learn by itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Reinforcement learning examples</h2>\n",
    "\n",
    "**DeepMind, 2015 & 2017: AlphaGo & AlphaZero**\n",
    "- One of the more famous RL successes: agent learning to play the game of Go and beating world champion Lee Sedol\n",
    "- In case you want to know more: <a href=\"https://www.youtube.com/watch?v=WXuK6gekU1Y\">documentary on YouTube</a>\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/alpha_go.png\" alt=\"AlphaGo\" style=\"width: 40%; margin-top: 1cm;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\"><a href=\"https://www.deepmind.com/research/highlighted-research/alphago\">DeepMind AlphaGo</a></p>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Reinforcement learning examples</h2>\n",
    "\n",
    "**OpenAI, 2019: hide-and-seek**\n",
    "- RL agents learning to play hide-and-seek in a multi-agent setting\n",
    "- Recommend to watch the <a href=\"https://openai.com/blog/emergent-tool-use/\">short video</a>\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/hide_and_seek.png\" alt=\"Hide and seek\" style=\"width: 40%; margin-top: 1cm;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\"><a href=\"https://openai.com/blog/emergent-tool-use/\">OpenAI</a></p>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Reinforcement learning examples</h2>\n",
    "\n",
    "**DeepMind & EPFL, 2022: tokamak control**\n",
    "- Shaping and maintaining high-temperature plasma within tokamak vessel is challenging\n",
    "- Requires high-dimensional, high-frequency, closed-loop control using magnetic actuator coils\n",
    "- Paper describes RL agent that was successfully trained as a magnetic controller\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/tokamak.png\" alt=\"RL for Tokamak\" style=\"width: 80%; margin-top: 1cm;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\"><a href=\"https://www.nature.com/articles/s41586-021-04301-9\">Paper</a>, <a href=\"https://www.spektrum.de/news/auf-verschlungenen-pfaden/1698480\"> EuroFusion</a></p>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Reinforcement learning examples</h2>\n",
    "\n",
    "**DeepMind, 2022: AlphaTensor**\n",
    "- Matrix multiplication is a very fundamental mathematical operation\n",
    "- Improving its computational efficiency can benefit many fields\n",
    "- RL agent discovered more computationally efficient algorithms than developed by humans\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/alpha_tensor.png\" alt=\"AlphaTensor\" style=\"width: 35%; margin-top: 1cm;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\"><a href=\"https://www.nature.com/articles/s41586-022-05172-4\">Paper</a></p>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>What is reinforcement learning?</h2>\n",
    "\n",
    "- **Application:** online optimal control, decision-making tasks\n",
    "- **Goal:** learn optimal behavior in given environment\n",
    "- **Trial-and-error learning:** agent takes actions in environment and collects rewards\n",
    "- We provide **minimal input**: reward function, state definition\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/rl_schematic.png\" alt=\"RL schematically\" style=\"width: 50%;margin-top: 1cm;\" />\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>An example: Pacman</h3>\n",
    "\n",
    "- For games it is typically easy to define what the state, actions, and rewards are\n",
    "    - **State:** where am I? Where are ghosts, snacks, cookies, walls? *(discrete set)*\n",
    "    - **Action:** up, down, left, right *(discrete set)*\n",
    "    - **Reward:** food (+), ghosts (-)\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/pacman.png\" alt=\"Pacman example\" style=\"width: 40%; margin-top: 1cm;\" />\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Another example: beam trajectory steering</h3>\n",
    "\n",
    "- Maximize integrated beam on target\n",
    "    - **State:** beam position somewhere in the line *(continuous variable)*\n",
    "    - **Action:** increase or decrease dipole kick angle, or strength *(continuous variable)*\n",
    "    - **Reward:** amount of beam on target\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/1d_beam_steering.png\" alt=\"1D beam steering\" style=\"width: 70%; margin-top: 0.5cm;\" />\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Today's lecture</h2>\n",
    "\n",
    "- There are **various RL algorithms** suitable for different types of tasks\n",
    "- Often the choice of algorithm depends on whether we deal with **discrete or continuous state-action spaces**\n",
    "\n",
    "\n",
    "- We will go through:\n",
    "    1. **Discrete states, discrete actions:** Q-learning with a lookup table\n",
    "    2. **Continuous states, discrete actions:** Q-learning with a neural network (deep Q-learning or DQN)\n",
    "    3. **Continuous states, continuous actions:** actor-critic algorithm $\\Rightarrow$ what we typically need to control accelerator systems ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Our environment</h2>\n",
    "\n",
    "- A small grid maze\n",
    "- There are fires and a target field\n",
    "- A player, or agent has to navigate through and find the target field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from qlearning.core import Maze\n",
    "\n",
    "env = Maze(height=3, width=5)\n",
    "env.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Take some actions\n",
    "env.plot()\n",
    "env.step(action='up')\n",
    "env.plot()\n",
    "env.step(action='right')\n",
    "env.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>RL definitions</h3>\n",
    "\n",
    "- **State:** player / agent position (<a style=\"color: #ff0000;\"> <strong>x</strong> </a>) whose coordinates are defined by a tuple $(x, y)$\n",
    "- **Action:** 'up', 'down', 'left', 'right'\n",
    "- **Reward:** every action comes with a reward, depending on the new state we end up in\n",
    "    - Taking a step into an empty field: -1\n",
    "    - Bumping into walls: -5\n",
    "    - Going through fire: -10\n",
    "    - Reaching the goal: +30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>RL is about taking the best decisions ... </h2>\n",
    "\n",
    "- Obviously there are **better and worse trajectories to reach the target**. \"Better\" and \"worse\" refer to **how much reward** we can collect along the way.\n",
    "- We will get back to that\n",
    "\n",
    "<p style=\"color: #e6541a;\"> <strong>Exercise 1</strong> </p>\n",
    "Using the reward definitions from the previous slide, try to calculate the cumulative rewards for the trajectories shown below. Can you tell which of the paths are equally good / bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"img/example_trajectories.png\" alt=\"Example trajectories\" style=\"width: 30%;\" />\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: #e6541a;\"> <strong> TO DO: give context </strong> </p>\n",
    "\n",
    "<h2>Compare different methods</h2>\n",
    "\n",
    "Smart learning vs. 'stupid' algorithms: explain how rl compares to other optimization algorithms\n",
    "\n",
    "- If we have a model, could solve analytically with dynamic programming\n",
    "- If we do not have a model, can use e.g. Monte Carlo methods, sampling entire trajectories of the graph from start to end\n",
    "- Reinforcement learning takes a different approach and uses sub-trajectories to update the ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: center; width:100%;\">\n",
    "    <h2>Part II: reinforcement learning formalism</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Markov process</h3>\n",
    "\n",
    "- A **memoryless random process** consisting of a **set of states $S$ and state transition probabilities**\n",
    "- **Markov property:** the process is memoryless, i.e. the future depends only on the present state, but not on how we got there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"img/markov_chain.png\" alt=\"Markov chain\" style=\"width: 45%;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">image by <a href=\"https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf\">D. Silver - Lecture on RL</a></p>\n",
    "    \n",
    "</center>\n",
    "\n",
    "$S = \\{\\text{Class 1, Class 2, Class 3, Facebook, Pub, Pass, Sleep}\\}$  \n",
    "*Note that \"Sleep\" is also called a **terminal state**, because once in it we will never leave it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Markov reward process</h3>\n",
    "\n",
    "- A Markov process that has in addition a **reward function** and a **discount factor** $\\gamma \\in [0, 1]$\n",
    "- **Return** $G_t$: sum of discounted future rewards\n",
    "\n",
    "$$G_t = \\sum_k \\gamma^k \\, r_{t+k}$$\n",
    "\n",
    "- $\\gamma$ controls the relative importance of immediate vs future rewards\n",
    "    - $\\gamma \\rightarrow 0$: we only care about immediate rewards\n",
    "    - $\\gamma \\rightarrow 1$: we care about rewards far in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"img/markov_reward_process.png\" alt=\"Markov reward process\" style=\"width: 50%;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">image by <a href=\"https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf\">D. Silver - Lecture on RL</a></p>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Markov decision process (MDP)</h3>\n",
    "\n",
    "- Extend Markov reward process by adding **decision making**: set of possible actions $A$ (= action space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"img/markov_decision_process.png\" alt=\"Markov decision process\" style=\"width: 50%;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">image by <a href=\"https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf\">D. Silver - Lecture on RL</a></p>\n",
    "    \n",
    "</center>\n",
    "\n",
    "\n",
    "<br> *N.B.: stochastic state transitions are still allowed (if we decide to go to the Pub, anything can happen).*  \n",
    "*Today we will work with fully deterministic MDPs only.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Episodic MDP**\n",
    "    - Each episode ends in a terminal state\n",
    "    - Return $G_t$ is the sum of discounted rewards collected from time $t$ till end of episode\n",
    "    - Episodes are independent\n",
    "\n",
    "\n",
    "- **Continuous MDP**\n",
    "    - Continues indefinitely: has no terminal states\n",
    "    - Very important that discount factor $\\gamma < 1$ to avoid infinite returns\n",
    "    - Also known as infinite horizon MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Policy $\\pi$</h3>\n",
    "\n",
    "- The policy defines the decision making or **behavior of the agent**\n",
    "- It is a **probability distribution over the state-action space**. You can also think of it as a mapping that assigns to each state-action pair $(s, a)$ a probability\n",
    "\n",
    "$$\\pi: S \\times A \\rightarrow [0, 1]$$\n",
    "\n",
    "\n",
    "- $S$ and $A$ are the state and action spaces, respectively\n",
    "    - For our maze: $S = \\{[0, 0], [0, 1], ..., [\\text{width}-1, \\text{height}-1]\\}$ and $A = \\{\\text{'up', 'down', 'left', 'right'}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"color: #e6541a;\"> <strong>Exercise 2</strong> </p>\n",
    "\n",
    "Let's get back to the maze! For now we do not care about optimal decisions. Instead, try to **implement a random policy**, i.e. every action $a \\in \\{\\text{'up', 'down', 'left', 'right'}\\}$ is picked with equal probability no matter what state the agent is in.\n",
    "\n",
    "**a)** Initialize a Maze with `height=3, width=2` and complete the `all_actions` list.\n",
    "\n",
    "**b)** Look at every step of the output: is the movement of the agent (<a style=\"color: #ff0000;\"> <strong>x</strong> </a>) and the rewards obtained consistent with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from qlearning.core import Maze\n",
    "\n",
    "env = Maze( # FILL HERE)\n",
    "env.plot(title='Initial state')\n",
    "\n",
    "all_actions = ['up', # ... FILL HERE]\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.random.choice(all_actions)\n",
    "    state, action, reward, new_state, done = env.step(action)\n",
    "    env.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>RL objective</h2>\n",
    "\n",
    "- **Find optimal behavior in a given environment:** in every state we want the agent to take the best action\n",
    "- This is also known as the **optimal policy** $\\pi^*$\n",
    "- Formally, $\\pi^*$ **maximizes the return** $G_t = \\sum_k \\gamma^k \\, r_{t+k}$, i.e. the cumulative sum of discounted future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **For our maze, RL will solve ...**\n",
    "    - For any given field that we are currently on (= state), what is the action that maximizes the sum of rewards collected over time?\n",
    "    - Or: from where I stand - how can I reach the target field with the **least steps and not going through fires** (if possible) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>RL taxonomy</h2>\n",
    "\n",
    "- There are many **different algorithms** for finding the optimal policy $\\pi^*$\n",
    "- They all have their pros and cons\n",
    "    - Often the **sample-efficiency** is crucial\n",
    "    - It tells us **how many interactions with the environment** (= how many data samples) we need to solve the RL problem\n",
    "    - E.g. for **accelerator systems**: we want to train the agent with **as little beam time as possible** as it is very expensive\n",
    "    \n",
    "    \n",
    "- **Today:** we are going to look at **Q-learning**. It is one of the **core ideas** of many RL algorithms, such as\n",
    "    - Deep Q-learning (DQN)\n",
    "    - Actor-critic methods (DDPG, TD3, SAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"img/rl_taxonomy.png\" alt=\"The RL algorithm zoo\" style=\"width: 55%;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">image by <a href=\"https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html\">Open AI - Spinning Up</a></p>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"img/sample_efficiency.png\" alt=\"Sample efficiency\" style=\"width: 70%; margin-top: 1cm;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">image adapted from <a href=\"https://rail.eecs.berkeley.edu/deeprlcourse/\">S. Levine, \"Deep Reinforcement Learning\" (lecture)</a></p>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"color: #b51f2a\">Intermediate summary</h2>\n",
    "\n",
    "- The goal of RL is to **make optimal decisions** (*take actions*) in an environment based on some observables (*state*)\n",
    "- **Example environments:** game, control system (e.g. fusion reactor, tuning accelerator parameters), trading, ...\n",
    "- The **quality of a decision** made is quantified by a **reward**\n",
    "- Through **trial-and-error** the RL agent collects rewards and can eventually learn the **best behavior** (*optimal policy* $\\pi^*$)\n",
    "- Formally this is described as a **Markov decision process (MDP)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: center; width:100%;\">\n",
    "    <h2>Part III: Q-learning</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Q-learning</h2>\n",
    "\n",
    "- Employs a **state-action value function**\n",
    "$$Q: S \\times A \\rightarrow \\mathbb{R}$$\n",
    "to solve the RL problem\n",
    "\n",
    "\n",
    "- The Q-value $Q(s, a)$ characterizes the **\"quality\" of the state-action pair** $(s, a)$\n",
    "    - Quality is measured as the **expected return** acting according to a certain policy\n",
    "    $$Q(s, a) = \\mathbb{E}[G_t | S_t = s, A_t = a]$$\n",
    "    - Reminder: $G_t = \\sum_k \\gamma^k \\, r_{t+k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Temporal difference (TD) learning**\n",
    "    - We can write the Q-value of $(s, a)$ as a sum of immediate reward $r$ plus discounted Q-value of the next state-action pair $(s', a')$\n",
    "    - *N.B.: we are acting greedily, hence the $\\text{max}$ operation*\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/backup_diag_qlearn.png\" alt=\"Q-learning backup diagram\" style=\"width: 60%; margin-top: 1cm;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Temporal difference (TD) rule**\n",
    "    - Q-values are initially unknown / random, but can be learned **iteratively** following the TD update rule\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\underbrace{[\\underbrace{r + \\gamma \\text{max}_{a'} \\, Q(s', a')}_{\\text{target}} - \\underbrace{Q(s, a)}_{\\text{current prediction}}]}_{\\text{TD error}},$$\n",
    "<p style=\"margin-left: 60px;\"> where $\\alpha$ is the learning rate</p>\n",
    "\n",
    "- Uses **trial-and-error experiences** collected by the agent in one step $(s, a, r, s')$: *state, action, reward, next state*.\n",
    "- This is the **core idea of Q-learning** and is a result of one of the Bellman equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Obtaining the optimal policy</h2>\n",
    "\n",
    "- Once Q-values have converged, it is easy to read off the optimal policy $\\pi^*$\n",
    "\n",
    "$$\n",
    "\\pi^*(s, a) = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        1 & \\mbox{if } a = \\text{argmax}_{a'} \\, Q(s, a') \\\\\n",
    "        0 & \\mbox{otherwise.}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "- This is also known as the **greedy policy**: it acts greedily in terms of expected return by assigning probability $1$ to the action that maximizes the Q-function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>How to implement Q-learning?</h2>\n",
    "\n",
    "- We need a way to **track and update the Q-value** for each state-action pair\n",
    "    - Traditional Q-learning: **Q-table**\n",
    "    - Deep Q-learning: **neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"img/qlearn_dqn.png\" alt=\"Q-learning vs DQN\" style=\"width: 45%;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">image by <a href=\"https://www.assemblyai.com/blog/reinforcement-learning-with-deep-q-learning-explained/\">AssemblyAI</a></p>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Some challenges</h2>\n",
    "\n",
    "- Reward engineering\n",
    "- Definition of the state, which is sometimes only partially observable\n",
    "- Hyperparameter tuning, making training stable\n",
    "- Exploration vs exploitation ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Exploration-exploitation trade-off</h3>\n",
    "\n",
    "- To learn the best policy in the most efficient manner, we need a **trade-off between exploration and exploitation**.\n",
    "- We have to **ensure that the agent keeps exploring** new actions during training and does not just always follow the path that provides the highest expected return\n",
    "- After all, Q-values might **not have converged yet**, there may still be better solutions than currently known\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/exploration_exploitation_tradeoff.jpg\" alt=\"Exploration-exploitation tradeoff\" style=\"width: 40%;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">image by <a href=\"https://inst.eecs.berkeley.edu/~cs188/sp20/assets/lecture/lec15_6up.pdf\">Berkeley AI course</a></p>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Q-learning with lookup table</h2>\n",
    "\n",
    "- The first Q-learning method we consider is using a **lookup table** to keep track of the Q-values during training\n",
    "- First, we **initialize** all Q-values **to 0**, then **update** the values according to the **TD rule**\n",
    "- *N.B.: this method works only for discrete sets of states and actions*\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/qtable.png\" alt=\"Q-table learning\" style=\"width: 80%;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\"></p>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from qlearning.plot_utils import print_qtable\n",
    "from qlearning.core import Maze, QLearner\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Initialize small maze environment\n",
    "env = Maze(width=2, height=2, fire_positions=[[1, 0]])\n",
    "_ = env.plot(add_player_position=False)\n",
    "\n",
    "# Initialize Q-learner with Q-table\n",
    "qtable_learner = QLearner(env, q_function='table')\n",
    "\n",
    "print('Initial Q-table')\n",
    "q_table = qtable_learner.q_func.get_q_table()\n",
    "print_qtable(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "qtable_learner.train(200)\n",
    "\n",
    "print('Q-table after 200 episodes')\n",
    "q_table = qtable_learner.q_func.get_q_table()\n",
    "print_qtable(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "qtable_learner.train(300)\n",
    "\n",
    "print('Q-table after 500 episodes')\n",
    "q_table = qtable_learner.q_func.get_q_table()\n",
    "print_qtable(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "qtable_learner.plot_training_evolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"color: #e6541a;\"> <strong>Exercise 3</strong> </p>\n",
    "\n",
    "**a)** Based on the evolution of the Q-values on the previous slide - would you consider the training to be complete after 500 episodes?\n",
    "\n",
    "**b)** Play with the number of episodes in the cell below until you find convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlearning.plot_utils import print_qtable\n",
    "from qlearning.core import Maze, QLearner\n",
    "\n",
    "np.random.seed(0)\n",
    "env = Maze(width=2, height=2, fire_positions=[[1, 0]])\n",
    "\n",
    "qtable_learner = QLearner(env, q_function='table')\n",
    "qtable_learner.train( # FILL HERE)\n",
    "qtable_learner.plot_training_evolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"color: #e6541a;\"> <strong>Exercise 4</strong> </p>\n",
    "\n",
    "**a)** Initialize a bigger maze `height=4`, `width=3`, with `fire_positions=[[2, 1], [2, 2]]` and use `q_function='table'` in the `QLearner` class. Then train it for `5000` episodes. \n",
    "\n",
    "**b)** Once the training is finished, plot the Q-values (you can just execute the cell, it is already complete).\n",
    "<br>*Next to each little arrow there is a number that denotes the Q-value of the corresponding action on that field. The red arrow indicates the action with the highest Q-value.*\n",
    "\n",
    "**c)** Finally, also plot the (greedy) policy by executing the third cell. Compare it to the Q-value plot to verify that we indeed always pick the action with the highest Q-value. Are there fields where two actions would be equally good (which ones)? Can you confirm that by looking at the Q-values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 4 a)\n",
    "from qlearning.core import Maze, QLearner\n",
    "\n",
    "env = Maze(width= # FILL HERE,\n",
    "           height= # FILL HERE,\n",
    "           fire_positions= # FILL HERE)\n",
    "\n",
    "qtable_learner = QLearner(env, q_function= # FILL HERE)\n",
    "qtable_learner.train( # FILL HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 4 b)\n",
    "from qlearning.plot_utils import plot_q_table\n",
    "\n",
    "q_table = qtable_learner.q_func.get_q_table()\n",
    "ax = env.plot(add_player_position=False)\n",
    "plot_q_table(q_table, env.target_position, env.fire_positions, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 4 c)\n",
    "from qlearning.plot_utils import plot_greedy_policy\n",
    "\n",
    "policy = qtable_learner.q_func.get_greedy_policy()\n",
    "ax = env.plot(add_player_position=False)\n",
    "plot_greedy_policy(policy, env.target_position, env.fire_positions, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"color: #e6541a;\"> <strong>Exercise 5</strong> (optional)</p>\n",
    "\n",
    "**a)** Using the same maze as above, reduce the punishment of going through fire by setting a `fire_reward=-2` (instead of -10) in the environment definition.\n",
    "\n",
    "**b)** Retrain the agent. How does the policy change compared to exercise 4? Can you explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlearning.core import Maze, QLearner\n",
    "from qlearning.plot_utils import plot_q_table, plot_greedy_policy\n",
    "\n",
    "# Env definition\n",
    "env = Maze(width=4, height=3, fire_positions=[[2, 1], [2, 2]], fire_reward= # FILL HERE)\n",
    "qtable_learner = QLearner(env, q_function='table')\n",
    "qtable_learner.train(5000)\n",
    "\n",
    "# Show Q-values\n",
    "q_table = qtable_learner.q_func.get_q_table()\n",
    "ax = env.plot(add_player_position=False)\n",
    "plot_q_table(q_table, env.target_position, env.fire_positions, ax=ax)\n",
    "\n",
    "# Show policy\n",
    "policy = qtable_learner.q_func.get_greedy_policy()\n",
    "ax = env.plot(add_player_position=False)\n",
    "plot_greedy_policy(policy, env.target_position, env.fire_positions, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Deep Q-learning (DQN)</h2>\n",
    "\n",
    "- **Main idea:** replace the Q-table by a simple, feed-forward neural network (Q-net)\n",
    "- Developed by DeepMind in 2013 to play Atari games *(<a href=\"https://arxiv.org/abs/1312.5602\">DQN paper</a>)*\n",
    "\n",
    "\n",
    "- A neural network (NN) is a **universal function approximator**, i.e. a fit model that can approximate any function (in theory)\n",
    "- The Q-net is a **mapping** from **state to Q-values** of all possible actions\n",
    "- Its parameters aka. weights are adjusted according to the **TD rule**, like the Q-table\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/qnet_neuron.png\" alt=\"Q-net\" style=\"width: 90%; margin-top: 0.5 cm;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\"></p>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"color: #e6541a;\"> <strong> Exercise 6</strong> </p>\n",
    "\n",
    "**a)** Repeat the same steps as above for the Q-table learner, but this time using `q_function='net'` as an argument in the `QLearner` class. Train it for `1500` episodes. This will take a couple of minutes.\n",
    "\n",
    "**b)** Compare the Q-values and policy to the one obtained with Q-table learning. Do you see differences? Why could that be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from qlearning.core import Maze, QLearner\n",
    "\n",
    "np.random.seed(0)\n",
    "env = Maze(width=4, height=3, fire_positions=[[2, 1], [2, 2]])\n",
    "qnet_learner = QLearner(env, q_function= # FILL HERE)\n",
    "qnet_learner.train( # FILL HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlearning.plot_utils import plot_q_table, plot_greedy_policy\n",
    "\n",
    "q_table = qnet_learner.q_func.get_q_table()\n",
    "ax = env.plot(add_player_position=False)\n",
    "plot_q_table(q_table, env.target_position, env.fire_positions, ax=ax)\n",
    "\n",
    "policy = qnet_learner.q_func.get_greedy_policy()\n",
    "ax = env.plot(add_player_position=False)\n",
    "plot_greedy_policy(policy, env.target_position, env.fire_positions, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Q-table vs DQN: pros, cons, and limitations</h2>\n",
    "\n",
    "- **Q-table**\n",
    "    - <p style=\"color: #008000;\">Easy to understand and validate</p>\n",
    "    - <p style=\"color: #A91E1E;\">Discrete $S$, $A$ spaces only</p>\n",
    "    - <p style=\"color: #A91E1E;\">Relatively small $S$, $A$ spaces only</p>\n",
    "\n",
    "    \n",
    "- **DQN**\n",
    "    - <p style=\"color: #008000;\">Big and continuous $S$ possible</p>\n",
    "    - <p style=\"color: #008000;\">No need to visit all states during training, because NNs are great interpolators</p>\n",
    "    - <p style=\"color: #A91E1E;\">Discrete and relatively small $A$</p>\n",
    "    - <p style=\"color: #A91E1E;\">Training may be unstable and harder to verify if we have reached convergence</p>\n",
    "    \n",
    "    \n",
    "    \n",
    "- Many real-world problems require **continuous $S$ *and* continuous $A$** $\\,\\,\\Rightarrow\\,\\,$ **actor-critic methods**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: center; width:100%;\">\n",
    "    <h2>Part IV: actor-critic methods</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Actor-critic scheme</h2>\n",
    "\n",
    "- **Two NNs**\n",
    "    \n",
    "- **Actor**\n",
    "    - Represents the policy $\\pi$ and is a mapping $\\pi: S \\rightarrow A$\n",
    "    - For each **continuous state**, it proposes a **continuous action**\n",
    "    - Learns from the critic\n",
    "    \n",
    "    \n",
    "- **Critic**\n",
    "    - Predicts Q-values and is a mapping $Q: S\\times A \\rightarrow \\mathbb{R}$\n",
    "    - Evaluates quality of $(s, a)$ pair proposed by critic\n",
    "    - Feeds back to the actor network: policy gradient rule\n",
    "\n",
    "\n",
    "*N.B.: networks are trained simultaneously*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"img/actor_critic.png\" alt=\"Actor-critic schematic\" style=\"width: 70%;\" />\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Critic parameters** $\\theta$ are updated according to the **TD rule**, just like in Q-learning\n",
    "- **Actor parameters** $\\chi$ are updated via **policy gradient**: for a given state $s$, how does the actor have to adjust its parameters to propose an action $a$ such that $Q(s,a)$ becomes larger?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Application in accelerator physics</h2>\n",
    "\n",
    "\n",
    "- We are going to consider a **trajectory steering problem** from CERN's **AWAKE**\n",
    "- <a href=\"https://www.nature.com/articles/s41586-018-0485-4\">Advanced Proton Driven Plasma Wakefield Acceleration Experiment</a>\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/awake.png\" alt=\"AWAKE\" style=\"width: 100%; margin-top: 1cm\"/> <p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">image by <a href=\"https://www.nature.com/articles/s41586-018-0485-4\">AWAKE Collaboration</a></p>\n",
    "\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>AWAKE electron beam line</h3>\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/awake_beamline.png\" alt=\"AWAKE beamline\" style=\"width: 50%; margin-top: 1cm;\"/>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>RL task definitions</h3>\n",
    "\n",
    "- **Goal:** given measured beam positions *(= continuous state)*, find best dipole corrector settings *(= continuous actions)* to keep beam close to the center of vacuum pipe\n",
    "\n",
    "\n",
    "- **State:** 10-d array of beam positions measured along the line\n",
    "- **Action:** 10-d array of dipole corrector strengths along the line\n",
    "- **Reward:** negative rms of beam offsets wrt. center\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/trajectory_task.png\" alt=\"Electron beam line steering task\" style=\"width: 50%; margin-top: 0.5cm;\"/>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"color: #e6541a;\"> <strong> Exercise 7</strong> </p>\n",
    "\n",
    "**Let's try to train an actor-critic agent on the AWAKE environment!** We are using the **DDPG** *(<a href=\"https://spinningup.openai.com/en/latest/algorithms/ddpg.html\">Deep Deterministic Policy Gradient</a>)* algorithm. It is one of the most basic actor-critic algorithms and hence also not the most stable one. Some improvements have been implemented in TD3.\n",
    "\n",
    "**a)** Run the following cell to initialize the AWAKE simulation environment `env` and a DDPG instance `agent`. Then reset the environment to misteer the beam, and plot the trajectory. The plot shows the beam position at the 10 BPMs installed along the electron beam line.\n",
    "\n",
    "**b)** Run the next cell to make a correction to the beam position. Run the cell multiple times and check how the trajectories before and after correction compare. Do you think the RL agent is doing a good job? Why or why not?\n",
    "\n",
    "**c)** Run the next cell to train the RL agent. Can you interpret the output plots showing evolution of agent training? Is the length of training appropriate or should we train with fewer / more steps?\n",
    "\n",
    "**d)** Check a few trajectories before and after correction now using the trained agent. How does it perform now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 7 a)\n",
    "from actor_critic.awake_env import e_trajectory\n",
    "from actor_critic.core import ClassicalDDPG, trainer, plot_training_log, run_correction\n",
    "\n",
    "env = e_trajectory()\n",
    "agent = ClassicalDDPG(state_space=env.observation_space, action_space=env.action_space)\n",
    "\n",
    "env.reset(init_outside_threshold=True)\n",
    "env.plot_trajectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 7 b)\n",
    "run_correction(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 7 c)\n",
    "training_log = trainer(env=env, agent=agent, n_steps=400)\n",
    "plot_training_log(env, agent, training_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 7 d)\n",
    "run_correction(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"color: #e6541a;\"> <strong> TO DO </strong> </p>\n",
    "\n",
    "<h2>Summary</h2>\n",
    "\n",
    "- ... all contents as relevant for the exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Comprehension questions</h2>\n",
    "\n",
    "- Do you have to increase or decrease the discount factor $\\gamma$ to put more emphasis on future rewards?\n",
    "- Why does Q-learning only work for discrete action spaces, both for the Q-table and the Q-net?\n",
    "- Why might some of the Q-table values converge earlier than others during training?\n",
    "- How can you obtain the optimal policy once you know the Q-values?\n",
    "- Is the Markov property fulfilled for the maze environment? Why, or why not? (*hint: think about whether it matters how you ended up in the state you are currently in*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Literature</h2>\n",
    "\n",
    "- R.S. Sutton and A.G. Barto, <a href=\"http://incompleteideas.net/book/RLbook2020.pdf\">\"Reinforcement learning - an introduction\"</a>, Book, 2nd edition, 2020.\n",
    "- S. Levine, <a href=\"https://rail.eecs.berkeley.edu/deeprlcourse/\">Deep Reinforcement Learning</a>, Lecture, UC Berkeley, 2022.\n",
    "- D. Silver, <a href=\"https://www.davidsilver.uk/teaching/\">Reinforcement learning</a>, Lecture, University College London (UCL), 2015.\n",
    "\n",
    "<h2>Python RL libraries</h2>\n",
    "\n",
    "- Stable baselines 3: <a href=\"https://github.com/DLR-RM/stable-baselines3\">github</a>, <a href=\"https://stable-baselines3.readthedocs.io/en/master/\">docs</a>\n",
    "- OpenAI gym: <a href=\"https://github.com/openai/gym\">github</a>, <a href=\"https://www.gymlibrary.dev/\">docs</a>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "rise": {
   "enable_chalkboard": true,
   "footer": "<p>Fachbereich Elektrotechnik und Informationstechnik (etit)   |   Institut für Teilchenbeschleunigung und Elektromagnetische Felder (TEMF)   |   Dr. Michael Schenk</p>",
   "header": "<img src='https://upload.wikimedia.org/wikipedia/de/thumb/2/24/TU_Darmstadt_Logo.svg/640px-TU_Darmstadt_Logo.svg.png' />",
   "scroll": true,
   "theme": "simple",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
